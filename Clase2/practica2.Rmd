---
title: "Practica 2 AID 2025"
output: html_document
---



## Funciones de Distribuciones en R

En R, las distribuciones de probabilidad tienen cuatro tipos de funciones:

1. **Funciones de densidad** (`d...`): Devuelven la densidad de probabilidad en un punto.
2. **Funciones de distribución acumulada** (`p...`): Devuelven la probabilidad acumulada hasta un punto.
3. **Funciones cuantil o inversas** (`q...`): Devuelven el valor de la variable aleatoria correspondiente a un cuantil dado.
4. **Funciones generadoras de números aleatorios** (`r...`): Se usan para generar números aleatorios (ya vistas en otra sección).

### **Distribución Bernoulli**

```{r}
sample(c(0,1), prob = c(0.5, 0.5), size=1, replace = T)
```

### **Distribución Binomial**

```{r binomial}
# Probabilidad de obtener exactamente 3 éxitos en 10 ensayos con p = 0.8
n <- 10
k <- 3
p <- 0.8
combinatorio_n_k <- factorial(n)/(factorial(k)*factorial(n-k) )
funcion_masa_en_k <- combinatorio_n_k * p**k * (1-p)**(n-k)
print(funcion_masa_en_k)
print(dbinom(3, size = 10, prob = 0.8))
```
```{r}

# Probabilidad acumulada P(X <= 4) en una binomial (12, 0.2)
probaTotalAcumulada <- 0
for (i in c(0:4)){
  probaTotalAcumulada <- probaTotalAcumulada + dbinom(i, size = 12, prob = 0.2)
}
print(probaTotalAcumulada)

print(pbinom(4, size = 12, prob = 0.2))

```

```{r}


# Cuantil 80% de una binomial (10, 0.5)
qbinom(0.8, size = 10, prob = 0.5)
```

### **Distribución Poisson**

```{r poisson}
# Probabilidad de obtener exactamente 4 eventos con media lambda = 2
dpois(4, lambda = 2)

# Probabilidad acumulada P(X <= 4) en una Poisson con lambda = 2
ppois(4, lambda = 2)

# Cuantil 90% de una Poisson con lambda = 2
qpois(0.90, lambda = 2)


```

### **Distribución Exponencial**

```{r exponential}
# Densidad en x = 1 de una distribución exponencial con tasa lambda = 2
dexp(1, rate = 2)

# Probabilidad acumulada P(X <= 1) en una exponencial con lambda = 2
pexp(1, rate = 2)

# Cuantil 75% de una exponencial con lambda = 2
qexp(0.75, rate = 2)

curve(dexp(x, rate = 2), from = 0, to = 3, lwd = 2, col = "steelblue", 
      ylab = "Densidad", xlab = "x", main = "PDF de la Exponencial (λ = 2)")

```

### **Ejemplo con una Distribución Normal**

```{r normal}
# Densidad de una normal estándar en x = 1
dnorm(1, mean = 0, sd = 1)

# Probabilidad acumulada P(X <= 1) en una normal estándar
pnorm(1, mean = 0, sd = 1)

# Cuantil 90 de una normal con media 2 y desviación estándar 3
qnorm(0.90, mean = 2, sd = 3)

# Probabilidad de que X esté entre -1 y 1 en una normal estándar
pnorm(1, mean = 0, sd = 1) - pnorm(-1, mean = 0, sd = 1)
```
```{r}
lambda <- 1
com <- -2
fin <- lambda+5
x <- com:fin
xnorm <- seq(com, fin, 0.01)
poisson_vals <- dpois(x, lambda)
normal_vals <- dnorm(xnorm, mean = lambda, sd = sqrt(lambda))

plot(x, poisson_vals, type = "h", col = "blue", lwd = 2,
     main = "Poisson vs Normal",
     ylab = "Densidad", xlab = "x")
lines(xnorm, normal_vals, col = "red", lwd = 2)
legend("topright", legend = c("Poisson", "Normal"), col = c("blue", "red"), lwd = 2)

```
###  **Ley de los grandes numeros**

```{r}
set.seed(123)

# Simular 600 lanzamientos de una moneda cargada (Bernoulli con p = 0.7)
n <- 600
x <- rbinom(n, size = 1, prob = 0.7)

# Calcular media acumulada
media_acumulada <- cumsum(x) / (1:n)

# Graficar
plot(media_acumulada, type = "l", col = "blue", lwd = 2,
     main = "Ley de los Grandes Números (LGN)",
     xlab = "Número de observaciones",
     ylab = "Media acumulada")
abline(h = 0.7, col = "red", lty = 2)  # Línea de la media teórica
legend("bottomright", legend = c("Media acumulada", "Media verdadera"),
       col = c("blue", "red"), lwd = 2, lty = c(1, 2))

```
### **Teorema central del limite**

```{r}
set.seed(220)

# Parámetros
n_muestras <- 100000    # cuántas muestras
tamaño_muestra <- 50   # tamaño de cada muestra

# Crear un vector para guardar las medias muestrales
medias <- numeric(n_muestras)

# Simular
for (i in 1:n_muestras) {
  muestra <- rexp(tamaño_muestra, rate = 1)  # Distribución exponencial (media = 1)
  medias[i] <- mean(muestra)
}

# Graficar histograma de las medias
hist(medias, breaks = 50, probability = TRUE, col = "skyblue",
     main = "Teorema Central del Límite (TCL)",
     xlab = "Medias muestrales")

# Agregar curva normal teórica
curve(dnorm(x, mean = 1, sd = 1/sqrt(tamaño_muestra)), add = TRUE, col = "red", lwd = 2)
legend("topright", legend = c("Medias muestrales", "Normal teórica"),
       col = c("skyblue", "red"), lwd = 2)

```

### **Distribución t de Student**

```{r student_t}
# Densidad de una t de Student con 10 grados de libertad en x = 2
dt(2, df = 10)

# Probabilidad acumulada P(X <= 2) en una t de Student con 10 grados de libertad
pt(2, df = 10)

# Cuantil 95% de una t de Student con 10 grados de libertad
qt(0.95, df = 10)
```
```{r}
library(shiny)

ui <- fluidPage(
  titlePanel("Comparación: t de Student vs Normal estándar"),
  sidebarLayout(
    sidebarPanel(
      sliderInput("df", "Grados de libertad (df):",
                  min = 1, max = 100, value = 1, step = 1)
    ),
    mainPanel(
      plotOutput("distPlot")
    )
  )
)

server <- function(input, output) {
  output$distPlot <- renderPlot({
    curve(dt(x, df = input$df), from = -5, to = 5, col = "blue", lwd = 2,
          ylab = "Densidad", xlab = "x", main = paste("t de Student (df =", input$df, ") vs Normal"))
    curve(dnorm(x), from = -5, to = 5, col = "red", lwd = 2, add = TRUE, lty = 2)
    legend("topright", legend = c("t de Student", "Normal estándar"),
           col = c("blue", "red"), lwd = 2, lty = c(1, 2))
  })
}

shinyApp(ui = ui, server = server)

```

### **Distribución Chi-cuadrado**

Casos de uso:

Pruebas de independencia (Chi-cuadrado)

Análisis de varianza (ANOVA)

Evaluación de bondad de ajuste

```{r chi_square}
# Densidad en x = 5 de una chi-cuadrado con 4 grados de libertad
dchisq(5, df = 4)

# Probabilidad acumulada P(X <= 5) en una chi-cuadrado con 4 grados de libertad
pchisq(5, df = 4)

# Cuantil 99% de una chi-cuadrado con 4 grados de libertad
qchisq(0.99, df = 4)
```
```{r}
curve(dchisq(x, df = 4), from = 0, to = 15, main = "Distribución Chi-cuadrado (grados de libertad = 4)")
curve(dchisq(x, df = 1), from = 0, to = 15, main = "Distribución Chi-cuadrado (grados de libertad = 1)")
curve(dchisq(x, df = 10), from = 0, to = 25, main = "Distribución Chi-cuadrado (grados de libertad = 10)")

```

### **Distribución F de Snedecor**

Casos de uso:

Comparación de varianzas

ANOVA (prueba F)

y otros..

```{r f_distribution}
# Densidad en x = 3 de una F con 5 y 10 grados de libertad
df(3, df1 = 5, df2 = 10)

# Probabilidad acumulada P(X <= 3) en una F con 5 y 10 grados de libertad
pf(3, df1 = 5, df2 = 10)

# Cuantil 95% de una F con 5 y 10 grados de libertad
qf(0.95, df1 = 5, df2 = 10)
```
```{r}
curve(df(x, df1 = 5, df2 = 10), from = 0, to = 5, main = "Distribución F (df1 = 5, df2 = 10)")
```



```{r distribuciones}
# Generación de muestras de distribuciones conocidas

muestra.unif1=runif(100) # genera una muestra uniforme en [0,1] de 100 datos
muestra.unif1 # devuelve la muestra generada
muestra.unif2=runif(200,min=2,max=5) # genera una muestra uniforme en [2,5] de 200 datos
muestra.unif2 # devuelve la muestra generada
muestra.norm.est=rnorm(30) # genera una muestra normal estándar de 30 datos
muestra.norm.est # devuelve la muestra generada
muestra.norm=rnorm(50,mean=10,sd=3) # genera una muestra normal(10,3) de 50 datos
muestra.norm # devuelve la muestra generada
muestra.gamma=rgamma(40,rate=2, shape=3) # genera una muestra gamma(2,3) de 40 datos
muestra.gamma # devuelve la muestra generada
muestra.f=rf(80,df1=5,df2=6) # genera una muestra F de Snedekor(5,6) de 80 datos
muestra.f # devuelve la muestra generada
muestra.exp=rexp(90,2) # genera una muestra exponencial(2) de 90 datos
muestra.exp # devuelve la muestra generada
muestra.chi=rchisq(70,df=4) # genera una muestra chi cuadrado con 4 grados de libertad de 70 datos
muestra.chi # devuelve la muestra generada

```


-   *EJEMPLO DE OPERACIONES SOBRE DF*

```{r operaciones_sobre_df}

# Cargo la base:
PACIENTE<-1:30
EDAD<-c(7, 7, 8, 7, 7, 10,7 ,7, 7 ,9, 9, 11, 7, 9,  9, 11, 12, 7, 11,  6,  8,  8,  7, 10,  7,  8, 10,  7,  9, 10)
SEXO<-c("M", "M", "M", "F", "M", "M" ,"M", "M", "M", "M", "M", "F", "M" ,"M" ,"F" ,"M", "M" ,"M" ,"M" ,"F" ,"F" ,"F", "F","F", "M" ,"M" ,"F", "F" ,"F" ,"M")  
PESO<-c(24.4, 23.6, 47.0, 24.0, 23.9, 41.0, 32.9, 22.4, 28.7, 31.4, 28.9, 51.2, 26.2, 58.5, 23.7, 25.5, 49.7, 39.6,42.5, 21.6, 38.0, 26.6, 20.4, 23.7, 21.4, 45.7, 51.3, 28.0, 26.9, 43.9)
TALLA<-c(1.2, 1.2, 1.4, 1.2, 1.2, 1.4, 1.3, 1.2, 1.3, 1.3, 1.3, 1.6, 1.3, 1.5, 1.3, 1.3, 1.7, 1.3, 1.5, 1.2, 1.3, 1.2, 1.2,1.3, 1.2, 1.4, 1.5, 1.3, 1.3, 1.5)
IMC<-c(16.94444, 16.38889, 23.97959, 16.66667, 16.59722, 20.91837, 19.46746, 15.55556, 16.98225, 18.57988,17.10059, 20.00000, 15.50296, 26.00000, 14.02367, 15.08876, 17.19723, 23.43195, 18.88889, 15.00000,22.48521, 18.47222, 14.16667, 14.02367, 14.86111, 23.31633, 22.80000, 16.56805, 15.91716, 19.51111)
PIMC<-c(7.97, 72.72, 97.08, 83.88, 45.85, 87.33, 96.57, 32.88, 80.77, 92.72, 55.54, 77.77, 70.70, 98.69,  3.25,2.07, 38.08, 98.75, 80.60, 39.97, 96.07, 71.06,  3.44,  2.02, 56.86, 98.99, 90.84, 57.50, 44.77, 84.89)
CC<-c(54, 52, 76, 63, 56, 78, 69, 52, 60, 69, 60, 75, 50, 88, 58, 73, 75, 76, 72, 52, 76, 54, 52, 56, 56, 78, 76, 57, 57, 76)
CatPeso<-c("N",  "N",  "OB", "N",  "N",  "SO", "OB", "N",  "N",  "SO", "N",  "N",  "N",  "OB", "D",  "D",  "N",  "OB","N" , "N",  "OB", "N",  "D",  "D",  "N",  "OB", "SO", "N",  "N",  "N" )
IMCin<-data.frame(PACIENTE,EDAD,SEXO,PESO,TALLA,IMC,PIMC,CC,CatPeso)
#View(IMCin)

head(IMCin) # muestra las seis primeras filas de datos y los nombres de las columnas
tail(IMCin)# muestra las seis últimas filas de datos y los nombres de las columnas

dim(IMCin)#30 9

rbind(head(IMCin),tail(IMCin))#une filas: las seis primeras filas con las seis últimas
cbind(IMCin$SEXO[1:5],IMCin$PESO[1:5]) #cuidado!
data.frame(IMCin$SEXO[1:5],IMCin$PESO[1:5]) #une columnas con sus 5 primeros elementos

table(IMCin$SEXO) # devuelve las frecuencias absolutas de las categorías de la variable

dist.conj=table(IMCin$CatPeso, IMCin$SEXO) # devuelve la distribución conjunta de las variables categoría de peso y sexo

Z= IMCin$TALLA*100 # guarda los datos de la talla en centímetros
mean(Z) # calcula la media
median(Z) # calcula la mediana 
quantile(Z, 0.75) # calcula el cualtil 75
quantile(Z, probs = seq(0, 1, 0.25)) # calcula los cuantiles 0, 25, 50, 75 y 100

is.na(Z) # indica los valores que faltan
W<-Z 
W[1]<-NA # asigna un valor perdido en la primera componente del vector W
is.na(W)
mean(W) # devuelve NA
mean(W,na.rm=T) # no considera los valores no disponibles
mean(na.omit(W)) # equivalente al anterior
median(W,na.rm=T) # otra funcion que requiere excluir los valores no disponibles
sd(W,na.rm=T) # otra funcion que requiere excluir los valores no disponibles


```

<br>

## Ejercicios

-   **Ejercicio 1** Calcular la media y mediana del vector x y el número
    de valores que están por debajo de la media y de la mediana, siendo
    x: x\<-c(1,5,7,9,3,5,6,2,4,7,5,6,9,8,6,2,6,1,4).

-   **Ejercicio 2** Escribir la función que calcula el módulo de un
    número real.

-   **Ejercicio 3** Usar for para hallar el resultado de dividir de
    manera consecutiva el número 1111 por los siguientes divisores (en
    este orden): 2, 3, 4, 5, 6.

-   **Ejercicio 4** Escribir una función que responda el signo del
    producto de dos factores dado, es decir "Positivo", o "Negativo", y
    en el caso que el producto sea 0 devuelva "Nulo".

-   **Ejercicio 5** Simular el lanzamiento de un dado.

-   **Ejercicio 6** Simular el lanzamiento de cuatro dados o de un mismo
    dado cuatro veces.

-   **Ejercicio 7** Supongamos una urna con 3 bolas blancas y 7 negras,
    simular la extracción de una bola (asignar, por ejemplo, el 1 a bola
    blanca y 0 a negra).

-   **Ejercicio 8** Simular 8 extracciones con reemplazamiento de la
    urna del ejercicio 7.

-   **Ejercicio 9** Calcular las frecuencias porcentuales de la variable
    sexo.

-   **Ejercicio 10** Calcular la distribución porcentual de la variable
    categoría peso por sexo.

<br>

------------------------------------------------------------------------

<br>

## Resueltos

<br>

-   **Ejercicio 1**

x=c(1,5,7,9,3,5,6,2,4,7,5,6,9,8,6,2,6,1,4)

m=mean(x) #calcula la media de los valores del vector

M=median(x) #calcula la mediana de los valores del vector

sum(x\<m) #calcula cuántos valores quedan por debajo de la media m

sum(x\<M) #calcula cuántos valores quedan por debajo de la mediana M

-   **Ejercicio 2**

OPCION 1

modulo\<-function(x){

if(x\>=0){

```         
return(x)
```

}

else{

```         
return(-x)
```

}

}

OPCION 2

modu = function(x) {

y = abs(x)

return(y) }

-   **Ejercicio 3**

cocientes=rep(0,5) \# inicializo el vector de cocientes en cero

y = 1111

for (i in 2:6){

y = y/i

cocientes[i-1]=y

}

print(cocientes)

-   **Ejercicio 4**

Definimos la función signo

signo = function(x,y) {

z = x\*y

res = ifelse(z==0,"Nulo",ifelse(z\>0,"positivo","negativo"))

return(res) }

signo(0,-2)

-   **Ejercicio 5**

sample(c(1,2,3,4,5,6), size=1)

-   **Ejercicio 6**

sample(c(1,2,3,4,5,6), size=4, replace=TRUE)

-   **Ejercicio 7**

sample(c(0,1), size=1, prob=c(0.3,0.7))

-   **Ejercicio 8**

sample(c(0,1), size=8, replace=TRUE, prob=c(0.3,0.7))

-   **Ejercicio 9**

*Cargo la base:*

PACIENTE\<-1:30

<br>

EDAD \<- c(7, 7, 8, 7, 7, 10,7 ,7, 7 ,9, 9, 11, 7, 9, 9, 11, 12, 7, 11,
6, 8, 8, 7, 10, 7, 8, 10, 7, 9, 10)

<br>

SEXO \<- c("M", "M", "M", "F", "M", "M" ,"M", "M", "M", "M", "M", "F",
"M" ,"M" ,"F" ,"M", "M" ,"M" ,"M" ,"F" ,"F" ,"F", "F","F", "M" ,"M"
,"F", "F" ,"F" ,"M")

<br>

PESO \<- c(24.4, 23.6, 47.0, 24.0, 23.9, 41.0, 32.9, 22.4, 28.7, 31.4,
28.9, 51.2, 26.2, 58.5, 23.7, 25.5, 49.7, 39.6,42.5, 21.6, 38.0, 26.6,
20.4, 23.7, 21.4, 45.7, 51.3, 28.0, 26.9, 43.9)

<br>

TALLA \<- c(1.2, 1.2, 1.4, 1.2, 1.2, 1.4, 1.3, 1.2, 1.3, 1.3, 1.3, 1.6,
1.3, 1.5, 1.3, 1.3, 1.7, 1.3, 1.5, 1.2, 1.3, 1.2, 1.2,1.3, 1.2, 1.4,
1.5, 1.3, 1.3, 1.5)

<br>

IMC \<- c(16.94444, 16.38889, 23.97959, 16.66667, 16.59722, 20.91837,
19.46746, 15.55556, 16.98225, 18.57988,17.10059, 20.00000, 15.50296,
26.00000, 14.02367, 15.08876, 17.19723, 23.43195, 18.88889,
15.00000,22.48521, 18.47222, 14.16667, 14.02367, 14.86111, 23.31633,
22.80000, 16.56805, 15.91716, 19.51111)

<br>

PIMC \<- c(7.97, 72.72, 97.08, 83.88, 45.85, 87.33, 96.57, 32.88, 80.77,
92.72, 55.54, 77.77, 70.70, 98.69, 3.25,2.07, 38.08, 98.75, 80.60,
39.97, 96.07, 71.06, 3.44, 2.02, 56.86, 98.99, 90.84, 57.50, 44.77,
84.89)

<br>

CC \<- c(54, 52, 76, 63, 56, 78, 69, 52, 60, 69, 60, 75, 50, 88, 58, 73,
75, 76, 72, 52, 76, 54, 52, 56, 56, 78, 76, 57, 57, 76)

<br>

CatPeso \<- c("N", "N", "OB", "N", "N", "SO", "OB", "N", "N", "SO", "N",
"N", "N", "OB", "D", "D", "N", "OB","N" , "N", "OB", "N", "D", "D", "N",
"OB", "SO", "N", "N", "N" )

<br>

IMCin \<- data.frame(PACIENTE,EDAD,SEXO,PESO,TALLA,IMC,PIMC,CC,CatPeso)

<br>

*combina las dos frecuencias en una salida*

sal.sexo=rbind(table(IMCin$SEXO),100*table(IMCin$SEXO)/length(IMCin\$SEXO))

<br>

*asigna nombre a las filas de la salida*

rownames(sal.sexo)=c("frec.abs","frec.porc")

<br>

*asigna nombre a las columnas de la salida*

colnames(sal.sexo)=c("Femenino", "Masculino")

<br>

*muestra la salida*

round(sal.sexo,2)

<br>

-   **Ejercicio 10**

<br>

*devuelve la distribución conjunta de las variables categoría de peso y
sexo*

dist_conj \<- table(IMCin$CatPeso, IMCin$SEXO)

<br>

*calculo los totales por sexo*

total \<- apply(dist_conj,2,sum)

<br>

*calcula la distribución porcentual por sexo*

dist_porc \<-
round(100\*cbind(dist_conj[,1]/total[1],dist_conj[,2]/total[2]),2)

<br>

*asigna nombre a las columnas de la distribución porcentual*

colnames(dist_porc) \<-c ("F(%)","M(%)")

<br>

*combina ambas distribuciones*

sal_conj \<- cbind(dist_conj,dist_porc)

<br>

*calcula el total de cada columna*

Totales \<- apply(sal_conj,2,sum)

<br>

*agrega una fila con los totales*

sal_fin \<- rbind(sal_conj,Totales)

<br>

*muestra la salida*

sal_fin

<br>

<br>



En el análisis previo de los datos es fundamental identificar y tratar dos problemas comunes:  
1. **Valores Perdidos:** La existencia de datos nulos o ausentes que pueden afectar la calidad de los modelos.  
2. **Outliers:** Observaciones que se desvían significativamente del comportamiento general de los datos y que pueden distorsionar análisis estadísticos.




# 1. Análisis Previo de los Datos

Antes de aplicar cualquier tratamiento, examinamos la estructura y características de los datos. Creamos un dataset de ejemplo que contiene tanto valores perdidos como posibles outliers.

```{r}
# Cargar librerías necesarias
library(tidyverse)
library(caret)
library(mice)
library(outliers)  # Para test de Grubbs y Dixon

var1 <- c(rnorm(1000, mean = 30, sd = 10))
var2 <- c(rnorm(1000, mean = 0, sd = 10))
var2 <- var1*3 + var2

p_na <- 0.05  # 5% proba de NA
p_outlier <- 0.005  # 0.5% proba de outlier

introducir_anomalies <- function(x, p_na, p_outlier) {
  for (i in seq_along(x)) {
    rnd <- runif(1)
    if (rnd < p_na) {
      x[i] <- NA
    } else if (rnd < p_na + p_outlier) {
      x[i] <- 10000 
    }
  }
  return(x)
}

var1 <- introducir_anomalies(var1, p_na, p_outlier)
var2 <- introducir_anomalies(var2, p_na, p_outlier)

set.seed(123)
datos <- tibble(
  id = 1:1000,
  variable1 = var1,
  variable2 = var2
)

# Mostrar resumen del conjunto de datos
summary(datos)
```

Se observa que existen valores NA en ambas variables y algunos valores extremos (10000 en ambas variables).



# 2. Manejo de Valores Perdidos

## 2.1. Identificación de Valores Perdidos

Para detectar la presencia de valores nulos, utilizamos funciones de R tanto de base como del tidyverse.

```{r}
# Contar valores perdidos en cada variable
datos %>% summarise(across(everything(), ~sum(is.na(.))))
```


- **Ojo!:**  
  - No identifica patrones o dependencias en la ocurrencia de NA (por ejemplo, si se agrupan por categorías).

```{r}
ggplot(datos, aes(x = variable1, y = variable2)) +
  geom_point(color = "blue", alpha = 0.6) +  # Scatter plot points
  geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear regression line
  labs(
    title = "Scatter Plot de Variable1 vs Variable2",
    x = "Variable 1",
    y = "Variable 2"
  ) +
  theme_minimal()

```
```{r}
datos_filtered <- datos %>% drop_na()

lower_q1 <- quantile(datos_filtered$variable1, 0.02)
upper_q1 <- quantile(datos_filtered$variable1, 0.98)
lower_q2 <- quantile(datos_filtered$variable2, 0.02)
upper_q2 <- quantile(datos_filtered$variable2, 0.98)

datos_filtered <- datos_filtered %>%
  filter(variable1 >= lower_q1, variable1 <= upper_q1,
         variable2 >= lower_q2, variable2 <= upper_q2)


ggplot(datos_filtered, aes(x = variable1, y = variable2)) +
  geom_point(color = "blue", alpha = 0.6) +  # Scatter plot points
  geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear regression line
  labs(
    title = "Scatter Plot sacando 2% mas grande y chico",
    x = "Variable 1",
    y = "Variable 2"
  ) +
  theme_minimal()
```


## 2.2. Imputación de Valores Perdidos

Existen diversas técnicas para imputar valores nulos. A continuación se muestran varias, cada una con sus pros y contras.

### 2.2.1. Eliminación de Observaciones con NA

Se eliminan las filas que contengan algún valor perdido.

```{r}
datos_omit <- na.omit(datos)
dim(datos_omit)
```

- **Ventajas:**  
  - Método sencillo y rápido.  
  - Evita introducir supuestos o sesgos derivados de la imputación.
- **Desventajas:**  
  - Puede reducir considerablemente el tamaño de la muestra si hay muchos NA.  
  - Se pierde información potencialmente útil.
  - Se pueden genenrar sesgos si los datos faltantes no son aleatorios.

### 2.2.2. Imputación Simple con la Media o Mediana

Sustituir los NA por la media (o mediana) de la variable.

```{r}
# Imputación con la media para variable1
datos <- datos %>% 
  mutate(variable1_impute_mean = ifelse(is.na(variable1), mean(variable1, na.rm = TRUE), variable1))

# Imputación con la mediana para variable2
datos <- datos %>% 
  mutate(variable2_impute_median = ifelse(is.na(variable2), median(variable2, na.rm = TRUE), variable2))
```

- **Ventajas:**  
  - Fácil de implementar y entender.  
  - Rápido de calcular.
- **Desventajas:**  
  - Reduce la variabilidad natural de la variable.  
  - Puede introducir sesgos, especialmente si la distribución es asimétrica.


### 2.2.3. Imputación Hot-Deck (Aleatoria)

Se imputan los valores perdidos seleccionando aleatoriamente un valor observado de la misma variable.

```{r}
# Función para imputación hot-deck
imputacion_hotdeck <- function(x) {
  missing <- is.na(x)
  x[missing] <- sample(x[!missing], sum(missing), replace = TRUE)
  return(x)
}

# Aplicar hot-deck a variable2
datos <- datos %>% mutate(variable2_impute_hotdeck = imputacion_hotdeck(variable2))
```

- **Ventajas:**  
  - Preserva la distribución original de la variable.  
  - Es simple y rápido de implementar.
- **Desventajas:**  
  - Introduce aleatoriedad que puede variar entre ejecuciones.  
  - No aprovecha información de correlación entre variables.




```{r}
# Cargar librerías necesarias
library(tidyverse)
library(caret)
library(mice)
library(outliers)  # Para test de Grubbs y Dixon
n <- 1000
# Generar variables iniciales
var1 <- rnorm(n, mean = 30, sd = 10)
var2 <- rnorm(n, mean = 0, sd = 10)
var2 <- var1 * 3 + var2

p_na <- 0.05       # 5% de probabilidad de NA
p_outlier <- 0.005 # 0.5% de probabilidad de outlier

# Función para introducir anomalías (NA y outliers)
introducir_anomalies <- function(x, p_na, p_outlier) {
  for (i in seq_along(x)) {
    rnd <- runif(1)
    if (rnd < p_na) {
      x[i] <- NA
    } else if (rnd < p_na + p_outlier) {
      x[i] <- 400 
    }
  }
  return(x)
}

var1 <- introducir_anomalies(var1, p_na, p_outlier)
var2 <- introducir_anomalies(var2, p_na, p_outlier)

set.seed(123)
datos <- tibble(
  id = 1:n,
  variable1 = var1,
  variable2 = var2
)

# Imputación con la mediana
datos <- datos %>%
  mutate(
    variable1_mediana = if_else(is.na(variable1),
                                median(variable1, na.rm = TRUE),
                                variable1),
    variable2_mediana = if_else(is.na(variable2),
                                median(variable2, na.rm = TRUE),
                                variable2)
  )

# Imputación con la media
datos <- datos %>%
  mutate(
    variable1_media = if_else(is.na(variable1),
                              mean(variable1, na.rm = TRUE),
                              variable1),
    variable2_media = if_else(is.na(variable2),
                              mean(variable2, na.rm = TRUE),
                              variable2)
  )



```



```{r}

# Suponiendo que 'datos_imputados' ya está creado y contiene las filas donde había NA
# Convertir a formato largo para cada método de imputación

original <- datos %>%
  select(id, variable1, variable2) %>%
  mutate(metodo = "Original")

mediana <- datos %>%
  select(id, variable1_mediana, variable2_mediana) %>%
  rename(variable1 = variable1_mediana,
         variable2 = variable2_mediana) %>%
  mutate(metodo = "Mediana")

media <- datos %>%
  select(id, variable1_media, variable2_media) %>%
  rename(variable1 = variable1_media,
         variable2 = variable2_media) %>%
  mutate(metodo = "Media")


# Unir todos los datasets en uno solo
datos_long <- bind_rows(original, mediana, media)
datos_long <- datos_long %>% filter(variable1<400, variable2<400 )


# Graficar solo los puntos con sus respectivos colores
ggplot(datos_long, aes(x = variable1, y = variable2, color = metodo)) +
  geom_point(alpha = 1) +
  labs(x = "Variable 1", y = "Variable 2", color = "Metodo de Imputacion") +
  theme_minimal()

```

# 3. Detección y Tratamiento de Outliers

Los outliers pueden afectar significativamente los resultados de un análisis. A continuación se muestran distintos métodos para detectarlos y tratarlos.

## 3.1. Detección Visual

### 3.1.1. Boxplot

El boxplot es una herramienta gráfica clásica para identificar posibles outliers.

```{r}
ggplot(datos, aes(y = variable1)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "Boxplot de variable1", y = "variable1") +
  theme_minimal()
```

### 3.1.2. Histograma y QQ-Plot

# **QQ-Plot: ¿Qué es y cómo interpretarlo?**

Un **QQ-Plot (Quantile-Quantile Plot)** es una herramienta gráfica utilizada en estadística para comparar la distribución de una variable con una distribución teórica, generalmente una distribución normal. Es útil para verificar si los datos siguen una distribución específica y detectar posibles desviaciones, como colas pesadas, asimetría o valores atípicos.

## **Cómo funciona un QQ-Plot**
1. **Cuantiles teóricos vs. Cuantiles muestrales**  
   - Se calculan los cuantiles de los datos observados.
   - Se comparan con los cuantiles teóricos de la distribución de referencia (por ejemplo, normal).
  
2. **Interpretación del gráfico**  
   - **Si los puntos siguen aproximadamente la línea azul**, los datos tienen una distribución cercana a la teórica (normal).
   - **Si los puntos se desvían en las colas** (extremos), indica presencia de valores extremos o colas pesadas.
   - **Si hay curvatura**, sugiere asimetría en los datos.


### **1️⃣ Tomar los datos**
   Se parte de un conjunto de datos \( X = \{x_1, x_2, \dots, x_n\} \) con \( n \) observaciones.

### **2️⃣ Ordenar los datos**
   - Se ordenan los datos en orden ascendente para obtener los valores \( x_{(1)}, x_{(2)}, \dots, x_{(n)} \), donde \( x_{(i)} \) es el \( i \)-ésimo valor ordenado.
   - Estos valores representan los **cuantiles muestrales**.

### **3️⃣ Calcular los cuantiles teóricos**
   - Se asume que los datos siguen una distribución normal teórica con media \( \mu \) y desviación estándar \( \sigma \).
   - Se calcula el **cuantil teórico esperado** para cada observación \( x_{(i)} \). Para esto, se usa la posición de cada dato en la muestra:
     \[
     p_i = \frac{i - 0.5}{n}
     \]
     donde \( p_i \) es el percentil estimado para cada dato.

   - Luego, se usa la función inversa de la distribución normal estándar \( \Phi^{-1}(p_i) \) para obtener los valores teóricos:
     \[
     q_i = \Phi^{-1}(p_i)
     \]
     donde \( q_i \) representa los **cuantiles teóricos** de una distribución normal estándar \( N(0,1) \).

### **4️⃣ Graficar los cuantiles muestrales contra los teóricos**
   - En el eje **X**, se colocan los cuantiles teóricos \( q_i \).
   - En el eje **Y**, se colocan los cuantiles muestrales \( x_{(i)} \).

### **5️⃣ Agregar la línea de referencia**
   - Se traza una línea recta \( y = ax + b \) ajustada por regresión, donde \( a \) y \( b \) corresponden a la media y la desviación estándar de la muestra.
   - Si los datos siguen una distribución normal, los puntos deberían alinearse con esta línea.



## **Ejemplo Numérico**
Supongamos que tenemos los siguientes datos:

\[
X = \{2.3, 2.9, 3.1, 3.8, 4.2, 4.8, 5.5\}
\]

1. **Ordenamos los datos**:  
   \[
   (2.3, 2.9, 3.1, 3.8, 4.2, 4.8, 5.5)
   \]

2. **Calculamos los cuantiles teóricos** (con \( n=7 \)) usando:

   \[
   p_i = \frac{i - 0.5}{n}, \quad i = 1,2,...,7
   \]

   Obtenemos los valores de \( p_i \):

   \[
   (0.07, 0.21, 0.36, 0.50, 0.64, 0.79, 0.93)
   \]

   Luego, aplicamos la función inversa de la normal estándar \( \Phi^{-1}(p_i) \) para obtener:

   \[
   (-1.47, -0.81, -0.36, 0.00, 0.36, 0.81, 1.47)
   \]

3. **Graficamos los puntos**:  
   - En **X**, ponemos los cuantiles teóricos.
   - En **Y**, los valores ordenados de la muestra.
   - Si los datos son normales, los puntos estarán cerca de una línea recta.



```{r}
# Histograma
ggplot(datos, aes(x = variable2)) +
  geom_histogram(fill = "salmon", color = "black", bins = 20) +
  labs(title = "Histograma de variable2", x = "variable2") +
  theme_minimal()

# QQ-Plot
ggplot(datos, aes(sample = variable2)) +
  stat_qq() +
  stat_qq_line(color = "blue") +
  labs(title = "QQ-Plot de variable2") +
  theme_minimal()
```

## 3.2. Detección mediante Estandarización (Z-scores)

Calculamos el z-score para identificar observaciones que se alejen más de 3 desviaciones estándar de la media.

```{r}
datos <- datos %>% mutate(z_variable1 = abs(scale(variable1)))
outliers_z <- datos %>% filter(z_variable1 > 3)
outliers_z
```

- **Ventajas:**  
  - Método simple y universal para distribuciones aproximadamente normales.
- **Desventajas:**  
  - No funciona bien para distribuciones muy asimétricas o con colas pesadas.

## 3.3. Detección con el Método del IQR

Se calculan el primer (Q1) y tercer cuartil (Q3) y se definen como outliers aquellos puntos que se encuentran fuera de \[Q1 - 1.5·IQR, Q3 + 1.5·IQR\].

```{r}
# Calcular cuartiles e IQR para variable2
q1 <- quantile(datos$variable2, 0.25, na.rm = TRUE)
q3 <- quantile(datos$variable2, 0.75, na.rm = TRUE)
iqr <- q3 - q1

# Definir límites
limite_inferior <- q1 - 1.5 * iqr
limite_superior <- q3 + 1.5 * iqr

# Filtrar outliers
outliers_iqr <- datos %>% filter(variable2 < limite_inferior | variable2 > limite_superior)
outliers_iqr
```

- **Ventajas:**  
  - No asume normalidad y es robusto frente a la presencia de outliers.
- **Desventajas:**  
  - Puede clasificar como outliers valores válidos en distribuciones muy sesgadas.

## 3.4. Test de Grubbs

Identifica el valor extremo más alejado de la media en una distribución normal (se remueven NA).

### **Test de Grubbs: Detección de Valores Atípicos en una Distribución Normal**

El **Test de Grubbs** es una prueba estadística utilizada para detectar valores atípicos en un conjunto de datos que sigue una distribución normal. Su objetivo es identificar si el valor extremo más alejado de la media es significativamente diferente del resto de los datos.




### **1. Formular la Hipótesis**
- **Hipótesis Nula (\(H_0\))**: No hay valores atípicos en los datos, es decir, todos los valores provienen de la misma distribución normal.
- **Hipótesis Alternativa (\(H_A\))**: Existe al menos un valor atípico en los datos.

### **2. Calcular la Estadística de Grubbs**
La estadística del test se calcula como:

\[
G = \frac{\max |X_i - \bar{X}|}{s}
\]

Donde:
- \( X_i \) es el valor más alejado de la media (\(\bar{X}\)).
- \( \bar{X} \) es la media de los datos.
- \( s \) es la desviación estándar muestral.

### **3. Comparar con el Valor Crítico**
El valor crítico de \( G \) se obtiene de una tabla de referencia o mediante software estadístico. Si \( G \) es mayor que el valor crítico para un nivel de significancia (\(\alpha\), típicamente 0.05), se rechaza \( H_0 \) y se considera el dato como un valor atípico.



### **4. Interpretación del Resultado**
- Si el valor p (\( p \)-value) es menor que el nivel de significancia (\(\alpha\)), rechazamos \(H_0\) y concluimos que el valor extremo es un atípico.
- Si el valor p es mayor que \(\alpha\), no se rechaza \(H_0\), lo que indica que no hay evidencia suficiente para considerar un valor como atípico.

### **5. Posibles Acciones**
- Si se detectan valores atípicos, se pueden eliminar o analizar su causa antes de tomar decisiones sobre los datos.
- Si no hay valores atípicos detectados, el conjunto de datos se considera limpio respecto a valores extremos.



### **Consideraciones**
- El test de Grubbs solo detecta un valor atípico por vez. Si hay varios valores atípicos, se debe aplicar iterativamente.
- Asume que los datos siguen una distribución normal. Si los datos no son normales, se deben usar otros métodos como el test de **Dixon** o el **IQR (Rango Intercuartílico)**.




```{r}
grubbs_result <- grubbs.test(na.omit(datos$variable1))
grubbs_result
```

### **Test de Dixon: Detección de Valores Atípicos en Muestras Pequeñas**

El **Test de Dixon** es una prueba estadística diseñada para identificar valores atípicos en conjuntos de datos pequeños o medianos. Es especialmente útil cuando el tamaño de la muestra es **menor o igual a 30**, ya que en muestras grandes otros métodos como el test de Grubbs o el análisis basado en cuartiles son más adecuados.



## **Paso a Paso del Test de Dixon**

### **1. Formular la Hipótesis**
- **Hipótesis Nula (\(H_0\))**: No hay valores atípicos en los datos.
- **Hipótesis Alternativa (\(H_A\))**: Al menos un valor en los datos es un valor atípico.

### **2. Calcular la Estadística de Dixon**
El test de Dixon calcula una estadística basada en la distancia entre el valor más extremo y el resto de la distribución. Existen diferentes fórmulas dependiendo de si se considera el menor o el mayor valor como sospechoso de ser un outlier. La estadística general es:

\[
Q = \frac{x_{\text{extremo}} - x_{\text{más cercano}}}{x_{\text{máximo}} - x_{\text{mínimo}}}
\]

Donde:
- \( x_{\text{extremo}} \) es el valor sospechoso de ser un outlier (mínimo o máximo del conjunto de datos).
- \( x_{\text{más cercano}} \) es el valor más cercano a \( x_{\text{extremo}} \).
- \( x_{\text{máximo}} \) y \( x_{\text{mínimo}} \) son los valores máximos y mínimos del conjunto de datos.

El valor de \( Q \) se compara con valores críticos tabulados. Si \( Q \) es mayor que el valor crítico correspondiente al tamaño de la muestra y nivel de significancia (\(\alpha\), típicamente 0.05), se considera que el valor extremo es un **outlier significativo**.



## **¿Por qué el Test de Dixon se usa para muestras de 30 o menos?**
1. **Precisión en muestras pequeñas:**  
   - Para tamaños mayores a 30, la probabilidad de obtener valores extremos debido al azar es más alta.
   - Métodos como Grubbs, IQR o tests basados en la desviación estándar son más apropiados para muestras grandes.

2. **Suposición de normalidad en muestras pequeñas:**  
   - El test de Dixon funciona mejor cuando la muestra sigue una distribución aproximadamente normal. En muestras grandes, la normalidad es más fácil de verificar y otros métodos pueden ser más efectivos.

3. **Valores críticos predefinidos:**  
   - Los valores críticos del test de Dixon están tabulados solo hasta muestras de tamaño 30.  
   - Para muestras grandes, la tabla no es tan confiable y se requieren métodos más robustos.



## **4. Implementación en R**
El código en R implementa el test de Dixon de la siguiente manera:

```{r}
# Instalar el paquete si no está instalado
install.packages("outliers")

# Cargar la librería
library(outliers)

# Contar la cantidad de datos no nulos (sin NA)
sample_size <- sum(!is.na(datos$variable1))

# Aplicar la prueba de Dixon solo si la muestra es <= 30
if (sample_size > 30) {
  submuestra <- sample(na.omit(datos$variable1), 30)  # Seleccionar una submuestra de 30 elementos
  dixon_result <- dixon.test(submuestra)  # Aplicar test de Dixon
  print(dixon_result)
} else {
  dixon_result <- dixon.test(na.omit(datos$variable1))  # Aplicar test directamente
  print(dixon_result)
}
```



## **5. Interpretación del Resultado**
- **Si el valor p (\( p \)-value) es menor que 0.05** → Se rechaza \(H_0\) y se concluye que hay al menos un valor atípico en los datos.
- **Si el valor p es mayor que 0.05** → No se puede afirmar que haya valores atípicos.




## 3.6. Tratamiento de Outliers

Una vez identificados los outliers, existen diversas estrategias para su tratamiento.

### 3.6.1. Eliminación de Outliers

Se eliminan las observaciones detectadas.

```{r}
datos_sin_outliers <- datos %>% 
  filter(!(variable2 < limite_inferior | variable2 > limite_superior))
dim(datos_sin_outliers)
```

- **Ventajas:**  
  - Elimina la influencia de valores extremos en el análisis.
- **Desventajas:**  
  - Puede llevar a pérdida de información y a sesgar la muestra si los outliers son parte de la variabilidad real.

### 3.6.2. Winsorización

Consiste en reemplazar los valores extremos por los percentiles de corte (por ejemplo, al 5% y 95%).

```{r}
winsorizacion <- function(x, lower = 0.05, upper = 0.95) {
  quantiles <- quantile(x, probs = c(lower, upper), na.rm = TRUE)
  x[x < quantiles[1]] <- quantiles[1]
  x[x > quantiles[2]] <- quantiles[2]
  return(x)
}

datos <- datos %>% 
  mutate(variable2_winsorizada = winsorizacion(variable2, 0.05, 0.95))
```

- **Ventajas:**  
  - Reduce el efecto de los outliers sin eliminarlos por completo.  
  - Conserva el tamaño de la muestra.
- **Desventajas:**  
  - Modifica la distribución de la variable.  
  - Puede ocultar la existencia de valores extremos reales.



# Dataset de ejercicio

En el dataset *airquality* se introducen valores extremos en la variable *Temp* (simulando outliers) y se mantienen los valores NA presentes en variables como *Ozone* y *Solar.R*.

```{r}
# Cargar las librerías necesarias
library(dplyr)
library(ggplot2)

# Cargar el dataset real
datos <- airquality

# Introducir outliers en 'Temp': se duplican el valor en el 5% de las observaciones
set.seed(123)
indices_extremos <- sample(1:nrow(datos), size = round(0.05 * nrow(datos)))
datos$Temp[indices_extremos] <- datos$Temp[indices_extremos] * 2

# Visualizar un resumen del dataset modificado
summary(datos)

# Nota: El dataset 'datos' presenta NA en variables como Ozone y Solar.R, lo cual es útil para la práctica.
```



# Ejercicios


### Ejercicio 1: Detección de NA  
- Calcular la proporción de valores perdidos en cada variable.  
- **Pista:** Utilizar `summarise` y `is.na()`.

### Ejercicio 2: Comparación de Imputaciones en *Ozone*  

  1. Imputar los valores NA en la variable *Ozone* usando la media.  
  2. Imputar los valores NA usando una técnica hot-deck (reemplazo aleatorio).  
  3. Comparar las distribuciones (mediante resúmenes estadísticos y gráficos) entre la versión original (casos completos) y las versiones imputadas.


### Ejercicio 3: Detección de Outliers con IQR en *Temp*  

  1. Detectar outliers en *Temp* utilizando el método IQR.  
  2. Generar un nuevo dataset sin los outliers.  
  3. Comparar la media y la desviación estándar antes y después de eliminar los outliers.

### Ejercicio 4: Winsorización vs. Eliminación en *Temp*  

  1. Aplicar winsorización (al 5% y 95%) a la variable *Temp*.  
  2. Comparar los resultados de winsorización con la eliminación de outliers (obtenida en el ejercicio 4) mediante resúmenes estadísticos y gráficos de densidad.

### Ejercicio 5: Test Estadísticos de Outliers en *Ozone*  
  1. Aplicar el test de Grubbs y el test de Dixon en *Ozone* para detectar outliers.  
  2. Discutir en qué situaciones es recomendable usar uno u otro test.

### Ejercicio 6: Uso de Imputación Múltiple con el dataset *mtcars*  
  1. Utilizar el dataset *mtcars* y modificar la variable *mpg* introduciendo NA de forma aleatoria (10% de los datos).  
  2. Aplicar imputación múltiple utilizando el paquete **mice** para imputar los valores NA.  
  3. Comparar la distribución de *mpg* antes y después de la imputación (mediante histogramas y resúmenes).



# Sección 3: Resoluciones Paso a Paso


## Ejercicio 1: Detección de NA

```{r}
# Calcular la proporción de NA en cada variable
proporcion_NA <- datos %>% summarise(across(everything(), ~mean(is.na(.))))
print(proporcion_NA)
```

## Ejercicio 2: Comparación de Imputaciones en *Ozone*

```{r}
# (a) Imputación con la media en 'Ozone'
datos <- datos %>% 
  mutate(Ozone_impute_mean = ifelse(is.na(Ozone), mean(Ozone, na.rm = TRUE), Ozone))

# (b) Imputación hot-deck para 'Ozone'
imputacion_hotdeck <- function(x) {
  missing <- is.na(x)
  x[missing] <- sample(x[!missing], sum(missing), replace = TRUE)
  return(x)
}

datos <- datos %>% mutate(Ozone_impute_hotdeck = imputacion_hotdeck(Ozone))

# Resumen estadístico y cálculo de la desviación estándar
cat("Imputación con la media:\n")
print(summary(datos$Ozone_impute_mean))
cat("SD:", sd(datos$Ozone_impute_mean, na.rm = TRUE), "\n\n")

cat("Imputación hot-deck:\n")
print(summary(datos$Ozone_impute_hotdeck))
cat("SD:", sd(datos$Ozone_impute_hotdeck, na.rm = TRUE), "\n\n")

# Comparación con la distribución original (solo casos sin NA)
datos_completos <- datos %>% filter(!is.na(Ozone))
cat("Datos originales (sin NA):\n")
print(summary(datos_completos$Ozone))
cat("SD:", sd(datos_completos$Ozone), "\n")
```


## Ejercicio 3: Detección de Outliers con IQR en *Temp*

```{r}
# Cálculo de cuartiles, IQR y límites para 'Temp'
q1 <- quantile(datos$Temp, 0.25, na.rm = TRUE)
q3 <- quantile(datos$Temp, 0.75, na.rm = TRUE)
iqr <- q3 - q1
limite_inferior <- q1 - 1.5 * iqr
limite_superior <- q3 + 1.5 * iqr

# Detectar outliers
outliers <- datos %>% filter(Temp < limite_inferior | Temp > limite_superior)
cat("Outliers en 'Temp':\n")
print(outliers)

# Crear un dataset sin outliers
datos_sin_outliers <- datos %>% filter(Temp >= limite_inferior & Temp <= limite_superior)

# Comparar media y desviación estándar
media_original <- mean(datos$Temp, na.rm = TRUE)
sd_original <- sd(datos$Temp, na.rm = TRUE)
media_sin <- mean(datos_sin_outliers$Temp, na.rm = TRUE)
sd_sin <- sd(datos_sin_outliers$Temp, na.rm = TRUE)

cat("Media y SD en 'Temp':\n")
print(c("Media original" = media_original, "SD original" = sd_original,
        "Media sin outliers" = media_sin, "SD sin outliers" = sd_sin))
```

## Ejercicio 4: Winsorización vs. Eliminación en *Temp*

```{r}
# Función para winsorización
winsorizacion <- function(x, lower, upper) {
  qnt <- quantile(x, probs = c(lower, upper), na.rm = TRUE)
  x[x < qnt[1]] <- qnt[1]
  x[x > qnt[2]] <- qnt[2]
  return(x)
}

# Aplicar winsorización en 'Temp'
datos <- datos %>% mutate(Temp_winsorizada = winsorizacion(Temp, 0.05, 0.95))

# Resúmenes estadísticos
cat("Resumen 'Temp' original:\n")
print(summary(datos$Temp))
cat("\nResumen 'Temp' winsorizada:\n")
print(summary(datos$Temp_winsorizada))
cat("\nResumen 'Temp' sin outliers (eliminación):\n")
print(summary(datos_sin_outliers$Temp))

# Comparación gráfica: gráficos de densidad
ggplot(datos, aes(x = Temp)) +
  geom_density(fill = "gray", alpha = 0.5) +
  labs(title = "Densidad de Temp - Original")

ggplot(datos, aes(x = Temp_winsorizada)) +
  geom_density(fill = "blue", alpha = 0.5) +
  labs(title = "Densidad de Temp - Winsorizada")

ggplot(datos_sin_outliers, aes(x = Temp)) +
  geom_density(fill = "green", alpha = 0.5) +
  labs(title = "Densidad de Temp - Sin Outliers")
```

## Ejercicio 5: Test Estadísticos de Outliers en *Ozone*

> **Nota:** Para estos tests es necesario el paquete **outliers**.

```{r}
library(outliers)

# Test de Grubbs para 'Ozone'
grubbs_result <- grubbs.test(na.omit(datos$Ozone))
cat("Resultado del test de Grubbs en 'Ozone':\n")
print(grubbs_result)

# Test de Dixon para 'Ozone'
sample_size <- sum(!is.na(datos$Ozone))
if (sample_size > 30) {
  submuestra <- sample(na.omit(datos$Ozone), 30)
  dixon_result <- dixon.test(submuestra)
  cat("Resultado del test de Dixon (muestra de 30) en 'Ozone':\n")
  print(dixon_result)
} else {
  dixon_result <- dixon.test(na.omit(datos$Ozone))
  cat("Resultado del test de Dixon en 'Ozone':\n")
  print(dixon_result)
}

# Comentario: El test de Grubbs es adecuado para detectar un único outlier asumiendo normalidad,
# mientras que el test de Dixon es útil en muestras pequeñas o cuando se sospecha de múltiples extremos.
```

## Ejercicio 6: Uso de Imputación Múltiple con el dataset *mtcars*

```{r}
library(mice)

# Copiar el dataset mtcars
data("mtcars")
mtcars_mod <- mtcars

# Introducir NA aleatorios en 'mpg' (10% de las observaciones)
set.seed(123)
na_indices <- sample(1:nrow(mtcars_mod), size = round(0.1 * nrow(mtcars_mod)))
mtcars_mod$mpg[na_indices] <- NA

# Resumen de 'mpg' con NA
cat("Resumen de 'mpg' en mtcars_mod (con NA):\n")
print(summary(mtcars_mod$mpg))

# Aplicar imputación múltiple con mice para la variable 'mpg'
imputacion_mtcars <- mice(mtcars_mod, m = 5, maxit = 50, method = 'pmm', seed = 500)

# Completar los datos imputados (se elige la primera imputación)
mtcars_imputed <- complete(imputacion_mtcars, 1)

# Ver resumen de 'mpg' después de la imputación
cat("Resumen de 'mpg' en mtcars_imputed:\n")
print(summary(mtcars_imputed$mpg))

# Histogramas comparativos
par(mfrow = c(1,2))
hist(mtcars$mpg, main = "Histograma de mpg - Original", xlab = "mpg", col = "gray", breaks = 10)
hist(mtcars_imputed$mpg, main = "Histograma de mpg - Imputado", xlab = "mpg", col = "blue", breaks = 10)
par(mfrow = c(1,1))
```



# 3. Normalización y Estandarización de Datos

## 3.1. ¿Por qué normalizar o estandarizar?

Muchos algoritmos de aprendizaje automático (como regresión logística, SVM, KNN y PCA) son sensibles a la escala de las variables. Cuando las variables tienen escalas muy diferentes, aquellas con valores más altos pueden dominar el cálculo de distancias o la función objetivo. Normalizar o estandarizar ayuda a:
  
- Asegurar que cada variable contribuya de manera equitativa.
- Mejorar la convergencia en algoritmos iterativos.
- Facilitar la interpretación de coeficientes en algunos modelos.

## 3.2. Normalización (Min-Max Scaling)

La normalización transforma los valores de una variable para que queden en el rango [0, 1] mediante la fórmula:

$$
x_{\text{norm}} = \frac{x - \min(x)}{\max(x) - \min(x)}
$$

### Implementación Manual

```{r}
# Crear datos de ejemplo
set.seed(123)
datos <- data.frame(variable = rnorm(100, mean = 50, sd = 10))

# Normalización manual
datos$variable_norm <- (datos$variable - min(datos$variable)) / (max(datos$variable) - min(datos$variable))
summary(datos$variable_norm)
```

### Usando el paquete `caret`

```{r}
library(caret)
preProc_range <- preProcess(datos, method = c("range"))
datos_norm <- predict(preProc_range, datos)
head(datos_norm)
```

La normalización es particularmente útil cuando se desea preservar la forma de la distribución, pero ajustando todos los valores a una escala común.

## 3.3. Estandarización (Z-score Scaling)

La estandarización reescala la variable para que tenga media 0 y desviación estándar 1, utilizando la fórmula:

$$
z = \frac{x - \mu}{\sigma}
$$

### Implementación Manual

```{r}
# Estandarización manual utilizando scale()
datos$variable_std <- scale(datos$variable)
summary(datos$variable_std)
```

### Usando el paquete `caret`

```{r}
preProc_std <- preProcess(datos, method = c("center", "scale"))
datos_std_caret <- predict(preProc_std, datos)
head(datos_std_caret)
```

La estandarización es muy útil para algoritmos basados en distancias, ya que asegura que las unidades de medida no influyan en el análisis.

# 4. Población y Muestra

## 4.1. Definiciones

- **Población**: Es el conjunto total de datos o elementos de interés sobre los cuales se desea hacer inferencias.
- **Muestra**: Es un subconjunto representativo extraído de la población que se utiliza para realizar estimaciones o pruebas estadísticas.

## 4.2. Tipos de Muestreo

### Muestreo Probabilístico

- **Aleatorio Simple**: Cada elemento de la población tiene la misma probabilidad de ser seleccionado.
- **Sistemático**: Se selecciona cada k-ésima observación a partir de un punto inicial aleatorio.
- **Estratificado**: La población se divide en estratos (subgrupos) y se selecciona una muestra aleatoria dentro de cada estrato para mantener la representatividad.
- **Por Conglomerados**: Se agrupa la población en conglomerados (por ejemplo, áreas geográficas) y se seleccionan algunos de estos conglomerados al azar.

### Muestreo No Probabilístico

- **Por Conveniencia**: Se elige la muestra con base en la accesibilidad y disponibilidad de los datos.
- **Por Cuotas**: Se fija una cuota para cada subgrupo basado en características conocidas.
- **Bola de Nieve**: Los sujetos iniciales recomiendan nuevos sujetos para la muestra.

## 4.3. Selección de Muestra en R

### Muestreo Aleatorio Simple

Utilizando la función `sample`, se extraen elementos al azar:

```{r}
# Seleccionar 100 valores al azar de 'variable' sin reemplazo
muestra_simple <- sample(datos$variable, size = 100, replace = FALSE)
head(muestra_simple)
```

### Muestreo Estratificado usando `caret`

Si se cuenta con una variable categórica para estratificar, se puede utilizar `createDataPartition` para asegurar que la distribución de los estratos se mantenga en la partición de entrenamiento y prueba.

```{r}
# Supongamos que añadimos una variable categórica para estratificar
datos$grupo <- sample(c("A", "B", "C"), size = nrow(datos), replace = TRUE)

library(caret)
set.seed(123)
trainIndex <- createDataPartition(datos$grupo, p = 0.7, list = FALSE)
train <- datos[trainIndex, ]
test <- datos[-trainIndex, ]

# Revisar la distribución de 'grupo' en la población, entrenamiento y prueba
table(datos$grupo)
table(train$grupo)
table(test$grupo)
```



## 5.1. Escalamiento Robusto
El escalamiento robusto se utiliza cuando la presencia de outliers puede distorsionar las medidas tradicionales (media y desviación estándar). Este método utiliza la mediana y el rango intercuartílico (IQR) para centrar y escalar los datos, según la fórmula:

$$
x_{\text{robust}} = \frac{x - \text{Mediana}(x)}{IQR(x)}
$$

```{r}
# Calcular el escalamiento robusto
mediana <- median(datos$variable, na.rm = TRUE)
iqr_val <- IQR(datos$variable, na.rm = TRUE)
datos$variable_robust <- (datos$variable - mediana) / iqr_val
summary(datos$variable_robust)
```

## 5.2. Transformación Logarítmica y Cuantil

### Transformación Logarítmica
Esta transformación es útil para reducir la asimetría en distribuciones sesgadas. Es recomendable aplicar logaritmo cuando los datos son estrictamente positivos.

```{r}
# Aplicar transformación logarítmica (se suma 1 para evitar log(0))
datos$variable_log <- log(datos$variable + 1)
summary(datos$variable_log)
```

### Transformación Basada en Cuantiles
Utilizando transformaciones como la de Yeo-Johnson o Box-Cox se pueden estabilizar las varianzas y aproximar la normalidad. Con el paquete `caret` se puede aplicar fácilmente:


### ** Box-Cox Transformation**
- Se aplica únicamente a datos **positivos** (mayores que 0).
- Tiene la forma matemática:

  \[
  X' =
  \begin{cases} 
  \frac{X^\lambda - 1}{\lambda}, & \text{si } \lambda \neq 0 \\
  \log(X), & \text{si } \lambda = 0
  \end{cases}
  \]

  Donde \(X'\) es el dato transformado y \(\lambda\) es un parámetro que se elige de manera óptima para normalizar los datos.

### ** Yeo-Johnson Transformation**
- Puede aplicarse a **valores negativos, positivos y ceros**.
- Tiene una fórmula distinta dependiendo del signo de \(X\):

  \[
  X' =
  \begin{cases} 
  \frac{(X + 1)^\lambda - 1}{\lambda}, & \text{si } X \geq 0, \lambda \neq 0 \\
  \log(X + 1), & \text{si } X \geq 0, \lambda = 0 \\
  \frac{-(|X| + 1)^{(2 - \lambda)} + 1}{2 - \lambda}, & \text{si } X < 0, \lambda \neq 2 \\
  -\log(|X| + 1), & \text{si } X < 0, \lambda = 2
  \end{cases}
  \]



```{r}
# Transformación usando el método Yeo-Johnson (aplicable a datos con valores negativos o cero)
preProc_quantile <- preProcess(datos, method = c("YeoJohnson"))
datos_quantile <- predict(preProc_quantile, datos)
head(datos_quantile)
```

## 5.3. Consideraciones Estadísticas en el Preprocesamiento

- **Normalización (Min-Max Scaling):** Preserva la forma de la distribución, pero es muy sensible a valores extremos.
- **Estandarización (Z-score Scaling):** Asume una distribución aproximadamente normal y puede ser influenciada por outliers, por lo que a veces es preferible el escalamiento robusto.
- **Transformaciones Logarítmicas o Cuantiles:** Ayudan a manejar datos sesgados, mejorando la estabilidad de la varianza y facilitando la modelación.

Visualizar la distribución antes y después de las transformaciones es fundamental para evaluar el impacto del preprocesamiento.

```{r}
library(ggplot2)
# Comparar la distribución original y la transformada
ggplot(datos, aes(x = variable)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "white") +
  labs(title = "Distribucion Original de 'variable'")

ggplot(datos, aes(x = variable_log)) +
  geom_histogram(bins = 30, fill = "salmon", color = "white") +
  labs(title = "Distribucion Transformada Logaritmicamente")
```

# 6. Variantes y Estrategias en Muestreo

## 6.1. Muestreo Sistemático

El muestreo sistemático consiste en seleccionar cada k-ésima observación de la población, lo cual puede ser eficiente si se conoce la estructura subyacente.

```{r}
# Calcular el intervalo k y seleccionar cada k-ésimo elemento
k <- floor(nrow(datos) / 20)
muestra_sistematico <- datos[seq(1, nrow(datos), by = k), ]
head(muestra_sistematico)
```

## 6.2. Muestreo por Conglomerados

En este método, se dividen los datos en conglomerados (por ejemplo, grupos geográficos o categóricos) y se selecciona aleatoriamente uno o varios conglomerados completos.

```{r}
# Suponiendo que 'grupo' representa conglomerados en el dataset:
# Asignar aleatoriamente grupos a los datos (si no existe esta variable)
datos$grupo <- sample(c("A", "B", "C", "D"), size = nrow(datos), replace = TRUE)

# Seleccionar aleatoriamente dos conglomerados
conglomerados_seleccionados <- sample(unique(datos$grupo), size = 2)
muestra_conglomerados <- subset(datos, grupo %in% conglomerados_seleccionados)
table(muestra_conglomerados$grupo)
```

## 6.3. Técnicas de Remuestreo (Bootstrapping)

El bootstrapping es una técnica de remuestreo utilizada en estadística para estimar la distribución de una estadística muestral sin necesidad de asumir una distribución teórica específica. Consiste en generar múltiples muestras de la misma población mediante remuestreo con reemplazo, permitiendo obtener una distribución empírica de la estadística de interés. Esto es útil para estimar errores estándar, intervalos de confianza y la variabilidad de estimadores.

### 6.3.1 Tipos de Bootstrapping

#### Bootstrapping No Paramétrico
Este método no hace suposiciones sobre la distribución de la población. Se basa en seleccionar repetidamente muestras con reemplazo de la muestra original y calcular la estadística de interés en cada una de ellas. Es ampliamente utilizado en situaciones en las que la distribución subyacente es desconocida o difícil de modelar.

#### Bootstrapping Paramétrico
A diferencia del enfoque no paramétrico, este método asume una distribución específica para los datos y genera muestras sintéticas a partir de parámetros estimados de la población. Se usa cuando se tiene información previa sobre la distribución de los datos y se desea modelar la variabilidad basándose en dicha distribución.

### 6.3.2 ¿Por qué usar Bootstrapping?
El bootstrapping es útil en diversas situaciones:
- Cuando el tamaño de la muestra es pequeño y los métodos asintóticos no son confiables.
- Cuando la distribución de la población no es conocida o es difícil de modelar.
- Para estimar la varianza de una estadística y construir intervalos de confianza sin necesidad de supuestos fuertes.
- Para validar modelos predictivos a través de técnicas como el bootstrap cross-validation.

### 6.3.3 Ejemplo con un Conjunto de Datos Real
Para ilustrar el uso del bootstrapping, utilizaremos el conjunto de datos `mtcars` de R. En este ejemplo, estimaremos la media del consumo de combustible (`mpg`) y su variabilidad usando bootstrapping no paramétrico.
```{r}
library(boot)

# Cargar los datos
datos <- mtcars
densidad <- density(datos$mpg)
print(mean(datos$mpg))
# Graficar la densidad
plot(densidad, main="Gráfico de Densidad de MPG", xlab="Millas por galón (mpg)", col="blue", lwd=2)
polygon(densidad, col=rgb(0, 0, 1, 0.2), border="blue") # Relleno con transparencia
```

```{r}


# Definir función para calcular la media del consumo de combustible
media_fn <- function(data, indices) {
  return(mean(data[indices], na.rm = TRUE))
}

# Aplicar bootstrapping con 1000 replicaciones
set.seed(123)
boot_result <- boot(data = datos$mpg, statistic = media_fn, R = 1000)
print(boot_result)
plot(boot_result)
```

El resultado de la función `boot()` nos proporciona:
- **Estadística original:** La media muestral de `mpg`.
- **Sesgo:** Diferencia entre la media de los valores bootstrapeados y la estadística original.
- **Error estándar:** Variabilidad estimada de la media.

Para obtener un intervalo de confianza del 95% basado en los percentiles de la distribución empírica obtenida:

```{r}
boot.ci(boot_result, type = "perc")
```

Este intervalo de confianza es útil para evaluar la incertidumbre de la estimación de la media de `mpg`.

### 6.3.4 Bootstrapping Paramétrico
Para realizar un bootstrapping paramétrico, asumimos que `mpg` sigue una distribución normal con media y desviación estándar estimadas de la muestra original. Generamos nuevas muestras de `mpg` basadas en esta distribución y aplicamos bootstrapping:

```{r}
param_bootstrap <- function(data, R) {
  mu <- mean(data, na.rm = TRUE)
  sigma <- sd(data, na.rm = TRUE)
  
  boot_samples <- replicate(R, mean(rnorm(length(data), mean = mu, sd = sigma)))
  return(boot_samples)
}

# Aplicamos el bootstrapping paramétrico
set.seed(123)
param_results <- param_bootstrap(datos$mpg, R = 1000)

# Estimamos el intervalo de confianza del 95%
quantile(param_results, probs = c(0.025, 0.975))
```

En este caso, generamos `R=1000` muestras sintéticas de `mpg` asumiendo una distribución normal. Luego, obtenemos el intervalo de confianza basado en los percentiles de la distribución generada.

## 6.4. Validación de la Representatividad de la Muestra

Comparamps las estadísticas descriptivas de la población y la muestra utilizada para asegurarnos de que la muestra sea representativa.

```{r}
library(dplyr)

# Estadísticas en la población
pop_stats <- datos %>%
  summarise(
    media = mean(mpg, na.rm = TRUE),
    mediana = median(mpg, na.rm = TRUE),
    sd = sd(mpg, na.rm = TRUE)
  )
print(pop_stats)

# Extraemos una muestra aleatoria simple
tamano_muestra <- 30
set.seed(123)
muestra_simple <- sample(datos$mpg, tamano_muestra, replace = FALSE)

# Estadísticas en la muestra aleatoria simple
sample_stats <- data.frame(
  media = mean(muestra_simple, na.rm = TRUE),
  mediana = median(muestra_simple, na.rm = TRUE),
  sd = sd(muestra_simple, na.rm = TRUE)
)
print(sample_stats)
```



# 7. División en Entrenamiento y Prueba

Una parte esencial del análisis predictivo es dividir el conjunto de datos en dos (o más) subconjuntos:  
- **Entrenamiento**: Donde se ajusta el modelo.  
- **Prueba**: Donde se evalúa el desempeño del modelo en datos no vistos.

Esta división ayuda a evitar el sobreajuste y proporciona una estimación realista de la capacidad predictiva.

## 7.1. División usando `caret`

El paquete `caret` facilita la creación de particiones de datos manteniendo la distribución de variables importantes (especialmente en el caso de variables categóricas).
```{r}
datos
```

```{r}
# Supongamos que 'datos' es nuestro dataset y 'grupo' es una variable de estratificación
set.seed(123)
library(caret)
datos$grupo <- sample(c("A", "B", "C", "D"), size = nrow(datos), replace = TRUE)

# Se utiliza createDataPartition para obtener índices de entrenamiento (70%)
trainIndex <- createDataPartition(datos$grupo, p = 0.7, list = FALSE)
train <- datos[trainIndex, ]
test <- datos[-trainIndex, ]

# Revisar la distribución de la variable estratificada en cada subconjunto
table(datos$grupo)
table(train$grupo)
table(test$grupo)
```

## 7.2. Ejemplo con un Modelo Simple

Se puede entrenar un modelo, por ejemplo, una regresión lineal, y evaluarlo en el conjunto de prueba.

```{r}
# Ajustar un modelo de regresión lineal usando el conjunto de entrenamiento
modelo_lm <- lm(mpg ~ ., data = train)

# Predicción en el conjunto de prueba
predicciones <- predict(modelo_lm, newdata = test)
prediccionesTrain <- predict(modelo_lm, newdata = train)

# Comparar las predicciones con los valores reales
resultados <- data.frame(Real = test$mpg, Predicho = predicciones)
resultadosTrain <- data.frame(Real = train$mpg, Predicho = prediccionesTrain)

head(resultados)

# Calcular error cuadrático medio (MSE)
mse <- mean((resultados$Real - resultados$Predicho)^2)
mseTrain <- mean((resultadosTrain$Real - resultadosTrain$Predicho)^2)

print(paste("MSE Test:", round(mse, 2)))
print(paste("MSE Train:", round(mseTrain, 2)))

```

# 8. Validación Cruzada

La validación cruzada es una técnica que permite evaluar el rendimiento de un modelo de forma más robusta al dividir los datos en varios pliegues (folds) y promediar el desempeño en cada uno.

## 8.1. k-Fold Cross Validation

Una de las formas más comunes es la validación cruzada k-fold, en la que el conjunto de datos se divide en k partes iguales. Se entrena el modelo k veces, cada vez usando k-1 partes para entrenamiento y la parte restante para prueba.

```{r}
# Definir un control de entrenamiento para validación cruzada k-fold (por ejemplo, k=10)
control_cv <- trainControl(method = "cv", number = 10)

# Entrenar el modelo utilizando validación cruzada
modelo_cv <- train(mpg ~ ., data = train, method = "lm", trControl = control_cv)
print(modelo_cv)
```

## 8.2. Leave-One-Out Cross Validation (LOOCV)

El LOOCV es un caso especial de k-fold donde k es igual al número de observaciones. Esto puede ser computacionalmente costoso, pero es útil para conjuntos de datos pequeños.

```{r}
# Configurar el control para LOOCV
control_loocv <- trainControl(method = "LOOCV")

# Entrenar el modelo utilizando LOOCV
modelo_loocv <- train(mpg ~ ., data = train, method = "lm", trControl = control_loocv)
print(modelo_loocv)
```

A continuación se presentan nuevos ejercicios utilizando datasets reales de R. Primero se exponen los enunciados de cada ejercicio y, posteriormente, se muestran sus resoluciones correspondientes.

# 9. Ejercicios

## Ejercicio 1: División y Evaluación de un Modelo con *mtcars*

**Enunciado:**  
Utiliza el dataset `mtcars` para dividirlo en un 70% de entrenamiento y un 30% de prueba. Ajusta un modelo de regresión lineal para predecir el consumo de combustible (`mpg`) en función de la potencia (`hp`) y el peso (`wt`). Evalúa el modelo en el conjunto de prueba calculando el error cuadrático medio (MSE).

## Ejercicio 2: Validación Cruzada k-Fold en un Modelo de Regresión

**Enunciado:**  
Con el conjunto de entrenamiento obtenido en el Ejercicio 1, realiza una validación cruzada de 10 pliegues para evaluar el rendimiento de un modelo de regresión lineal que prediga `mpg` a partir de `hp` y `wt`. Reporta el RMSE promedio obtenido durante la validación.

## Ejercicio 3: Comparación de Técnicas de Preprocesamiento con *airquality*

**Enunciado:**  
Utiliza el dataset `airquality` y enfócate en la variable `Ozone`.  
1. Elimina los valores faltantes de dicha variable.  
2. Aplica dos técnicas de preprocesamiento:  
   - **Normalización (Min-Max Scaling):** transforma los datos para que sus valores estén entre 0 y 1.  
   - **Estandarización (Z-score Scaling):** centra los datos en 0 y los escala de acuerdo con su desviación estándar.  
3. Visualiza, mediante histogramas, las distribuciones de la variable original, la normalizada y la estandarizada. Comenta las diferencias observadas.

# 9. Resoluciones

## Resolución Ejercicio 1

```{r}
# Utilizando el dataset mtcars
data(mtcars)
set.seed(123)

# División del dataset en 70% entrenamiento y 30% prueba
indices <- sample(1:nrow(mtcars), size = 0.7 * nrow(mtcars))
train_ex1 <- mtcars[indices, ]
test_ex1  <- mtcars[-indices, ]

# Ajustar el modelo de regresión lineal: mpg ~ hp + wt
modelo_ex1 <- lm(mpg ~ hp + wt, data = train_ex1)

# Realizar predicciones en el conjunto de prueba
predicciones_ex1 <- predict(modelo_ex1, newdata = test_ex1)

# Calcular el error cuadrático medio (MSE)
mse_ex1 <- mean((test_ex1$mpg - predicciones_ex1)^2)
print(paste("MSE del modelo:", round(mse_ex1, 2)))
```

## Resolución Ejercicio 2

```{r}
library(caret)
set.seed(123)

# Definir control de validación cruzada de 10 pliegues
control_ex2 <- trainControl(method = "cv", number = 10)

# Entrenar el modelo con validación cruzada utilizando el conjunto de entrenamiento
modelo_ex2 <- train(mpg ~ hp + wt, data = train_ex1, method = "lm", trControl = control_ex2)

# Mostrar los resultados de la validación cruzada
print(modelo_ex2)
# El RMSE promedio obtenido se encuentra en modelo_ex2$results$RMSE
```

## Resolución Ejercicio 3

```{r}
library(ggplot2)
data(airquality)

# Seleccionar la variable Ozone y eliminar valores NA
datos_ex3 <- na.omit(airquality[, "Ozone", drop = FALSE])

# Aplicar Normalización (Min-Max Scaling)
datos_ex3$Ozone_norm <- (datos_ex3$Ozone - min(datos_ex3$Ozone)) / 
                        (max(datos_ex3$Ozone) - min(datos_ex3$Ozone))

# Aplicar Estandarización (Z-score Scaling)
datos_ex3$Ozone_std <- scale(datos_ex3$Ozone)

# Visualizar las distribuciones

# Histograma de la distribución original
p1 <- ggplot(datos_ex3, aes(x = Ozone)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "white") +
  labs(title = "Distribucion Original de Ozone")

# Histograma de la distribución normalizada
p2 <- ggplot(datos_ex3, aes(x = Ozone_norm)) +
  geom_histogram(bins = 30, fill = "lightgreen", color = "white") +
  labs(title = "Distribucion Normalizada (Min-Max)")

# Histograma de la distribución estandarizada
p3 <- ggplot(datos_ex3, aes(x = Ozone_std)) +
  geom_histogram(bins = 30, fill = "salmon", color = "white") +
  labs(title = "Distribucion Estandarizada (Z-score)")

# Mostrar las gráficas
print(p1)
print(p2)
print(p3)
```
